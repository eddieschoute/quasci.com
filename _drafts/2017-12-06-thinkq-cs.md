---
layout:     post
title:      Classical vs Quantum Computation
date:       2018-03-12 16:38:05
summary:    ThinkQ conference experience
categories: conference algorithms advantage
---

The IBM ThinkQ conference was held recently in New York with a focus on near-term quantum computing applications.
It seems that businesses that businesses have successfully been developing larger quantum computers
---we're at around 50 qubits now!--- but are now looking for the "killer app" of small quantum computers.
There were some variations on the "what to do with your quantum computer" theme
and I will talk about some of the applications that were discussed.

All talks and recordings of them are available at the [online schedule](https://www.research.ibm.com/ibm-q/thinkq/agenda.html).

A _quantum advantage_ refers to some applications where a quantum computer performs some computation
that a classical computer currently cannot perform.
Previously known as _quantum supremacy_, it has now been renamed after an internal discussion
within the community about its political correctness.
See e.g. one of the most heated [discussions](https://scirate.com/arxiv/1705.06768) I've seen on Scirate,
which also touches on the Latin origin of the term _ancilla_ ("housemaid", colloquially: helper qubit).
While almost certainly an internet troll, _ancilla the supremacist_ has become somewhat of a joke in my environment,
so I guess it has served its purpose.

So far we actually do not know unconditionally if quantum computing is actually more powerful than classical
(i.e. $BQP \not\subseteq BPP$).
But through the problems of Boson Sampling[^boson1] and Instantaneous Quantum Polynomial-time (IQP) circuits[^iqp1]
we do know that the polynomial hierarchy ($PH$) must collapse if classical computers can solve them efficiently.
(Simulation and visitor (Hakop)


## Simulating Quantum Processes
One side of the discussion is determining which quantum processes can be efficiently simulated
by a classical computer.
We recently had Hakop Pashayan vist QuICS, who revealed to us some of the intricacies involved in
this process.
In their paper Hakop et al. explain the concept of ε-simulation[^hakop1].
For any quantum circuit $\mathcal C$ there exists some probability distribution $\mathcal P_\mathcal C$ over the outcomes $X=(X_1, X_2, …, X_k)$,
which is just a classical random variable.
Any noiseless circuit $\mathcal C$ can be described as starting with the product state $ρ_1 ⊗ … ⊗ ρ_n$
on $n$ qubits,
followed by some unitary operation $U$, and finally measuring qubits $1$ through $k$.
For example, one could ask the question what is the probability of measuring $(X_0, X_1) = (1,0)$ ignoring $X_2,…X_k$?
Or in other words, what is $\mathcal P(X_0 = 1, X_1 = 0)$?
An algorithm that can produce the answer to this (and similar) questions is called a _strong simulator_.
This is quite a powerful notion, since it is more powerful than a theoretical quantum computer
which can only produce a sample from the output distribution.
A slightly weaker notions is that of _weak simulation_:
output a sample in accordance with the output distribution $\mathcal P_\mathcal C$.
Even constructing a _weak simulator_ is probably too lofty of a goal,
because no real quantum computer will be completely noiseless
and thus cannot sample exactly from $\mathcal P_\mathcal C$.

![A general quantum circuit]({{ site.url }}/img/quantum-simulation-circuit.svg)\\
_A general quantum circuit, with $n$ product state inputs, a unitary evolution,
and then measurements on $k$ of the qubits._
{:.center}

### ε-Simulation
Hakop et al. introduce the notion of ε-simulation,
which allows the simulator to make some ε-sized error in the $L_1$ distance.

**Definition:** $L_1$ norm of discrete probability distributions
: For a discrete probability distribution $\mathcal P$ the $L_1$ norm is defined as

  $$\norm{\mathcal P}_1 = \sum_{i \in \mathcal P} \mathcal{P}(i).$$


  The $L_1$ distance between two discrete probability distributions $\mathcal P$ and $\mathcal Q$
  is then
  
  $$\norm{\mathcal{P} - \mathcal{Q}}_1 = \sum_{i ∈ P} \mathcal P(i) - \mathcal Q(i)$$

  which just takes the vector difference of the two probability distributions.
  (This notion also generalises to other norms, such as the $L_2$ norm and $L_\infty$ norm.)

**Definition:** ε-sampling [^hakop1]
: Let $\mathcal P$ be a discrete probability distribution.
  We say that a classical device or algorithm can ε-sample $\mathcal P$ iff for any $ε>0$
  it can sample from a probability distribution $P^ε$ such that
  $\norm{\mathcal P - \mathcal P^ε}_1 ≤ ε$.
  In addition, its run-time should scale polynomially in $1/ε$.

With the definition of ε-sampling, we can say that an algorithm can ε-simulate a quantum circuit $\mathcal C$
if it can ε-sample from the associated probability distribution $\mathcal P_\mathcal C$.
A result of Hakop et al.[^hakop1] is that an ε-simulator of $\mathcal C$ is indistinguishable from
$\mathcal C$ and also is efficient due to the polynomial run-time constraints.
Not only that, but it is also _necessary_ to be an ε-simulator for any kind of simulation scheme
to be efficient and indistinguishable from $\mathcal C$[^hakopscenario].


### Poly-Boxes and Simulations
To be able to ε-simulate a circuit $\mathcal C$ it is first necessary to estimate the probabilities
for some outcomes of the its output probability distribution $\mathcal P_\mathcal C$.
A _poly-box_ is a construction that estimates such probabilities in polynomial time.
It is not possible to efficiently estimate probabilities for general quantum circuits using 
a classical computer, but it may be possible to construct poly-boxes for certain restricted circuit
families.

**Definition:** Poly-box
: A poly-box over a family of quantum circuits $\mathbb S = \set{\mathcal C_a \middle| a ∈ \Sigma^*}$
with the associated family of probability distributions
$\mathbb P = \set{\mathcal P_\mathcal C \middle| \mathcal C∈ \mathbb A}$
for $Σ$ some finite alphabet.


## Polynomial Hierarchy

*[QuICS]: Joint Center for Quantum Information and Computer Science
*[product state]: Uncorrelated quantum state.
*[alphabet]: A set of characters. For example {0,1} is the binary alphabet. Usually combined with * to indicate zero or more repetitions of characters in the alphabet.

[^hakop1]: Pashayan, Hakop, Stephen D. Bartlett, and David Gross. "From estimation of quantum probabilities to simulation of quantum circuits." [arXiv: 1712.02806 [quant-ph]](https://arxiv.org/abs/1712.02806) (2017).
[^hakopscenario]: Hakop et al.[^hakop1] describe a specific hypothesis testing scenario for which they show this two-way implication.


## Quantum Advantage

### Boson Sampling

### Instantaneous Quantum Polynomial-time (IQP)
A second approach to showing a quantum advantage referred to as IQP is
to perform a diagonal unitary ($D$) on an input in the Hadamard basis ($\ket +, \ket -$)
that is converted back to the 'normal' computational basis ($\ket 0, \ket 1$) afterwards.
The output distribution of the circuit $\mathcal C = H^{\otimes n} D H^{\otimes n}$
turns out to be difficult to simulate for classical computers under suitable hardness assumptions.
[^iqpnoise]

[^boson1]: Aaronson, Scott, and Alex Arkhipov. "The computational complexity of linear optics." Proceedings of the forty-third annual ACM symposium on Theory of computing. ACM, 2011. {% include doi.html doi='10.1145/1993636.1993682' %}
[^iqp1]: Bremner, Michael J., Richard Jozsa, and Dan J. Shepherd. "Classical simulation of commuting quantum computations implies collapse of the polynomial hierarchy." Proceedings of the Royal Society of London A: Mathematical, Physical and Engineering Sciences. The Royal Society, 2010. {% include doi.html doi='10.1098/rspa.2010.0301' %}
[^iqpnoise]: Bremner, Michael J., Ashley Montanaro, and Dan J. Shepherd. "Achieving quantum supremacy with sparse and noisy commuting quantum computations." Quantum 1 (2017): 8. {% include doi.html doi='10.22331/q-2017-04-25-8' %}


Ashley talk
Ryan talk
Nathan wiebe
Earl Campbell
Micheal Bremner


## Applications
On the first day we had a high-level talk by Aram Harrow discussing some of the issues that
we face when finding applications that should exceed classical computers.
Usually computers are used for processing some amount of data, and reading all data will take at least $\Omega(n)$.
Therefore, if you do not have your data stored in some quantum format,
some obvious algorithms, such as searching through a database with Grover search,
will actually be bound by this data read-out.
For example, if we wish to find a 1 in a pile of data, it takes $O(\sqrt n)$ after
reading out all bits in $O(n)$, resulting in just an $O(n)$ algorithm.
Obviously, we would rather distribute this over classical data processors
since that would have the same asymptotics and be much faster.

Aram Harrow
Eddie Farhi
Andrew Childs
Robin Kothari
Garnet Chan

## Conclusion
Talks at qip 2018: bravyi gosset, koenig; Neville et al.; Clifford & Clifford
